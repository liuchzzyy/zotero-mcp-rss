"""
LLM client for Zotero MCP.

Provides unified interface for calling LLM APIs (DeepSeek, OpenAI, Gemini)
to analyze research papers and generate structured notes.
"""

import asyncio
import logging
import os
from typing import Any, Literal

logger = logging.getLogger(__name__)


# -------------------- Provider Configuration --------------------


PROVIDERS = {
    "deepseek": {
        "base_url": "https://api.deepseek.com",
        "default_model": "deepseek-chat",
        "api_style": "openai",  # OpenAI-compatible API
        "env_key": "DEEPSEEK_API_KEY",
        "env_base_url": "DEEPSEEK_BASE_URL",
        "env_model": "DEEPSEEK_MODEL",
    },
    "openai": {
        "base_url": "https://api.openai.com/v1",
        "default_model": "gpt-4o-mini",
        "api_style": "openai",
        "env_key": "OPENAI_API_KEY",
        "env_base_url": "OPENAI_BASE_URL",
        "env_model": "OPENAI_MODEL",
    },
    "gemini": {
        "base_url": "https://generativelanguage.googleapis.com",
        "default_model": "gemini-1.5-flash",
        "api_style": "google",
        "env_key": "GEMINI_API_KEY",
        "env_base_url": "GEMINI_BASE_URL",
        "env_model": "GEMINI_MODEL",
    },
}


# -------------------- Analysis Template --------------------


ANALYSIS_TEMPLATE = """ä½ æ˜¯ä¸€ä½ä¸“ä¸šçš„ç§‘ç ”æ–‡çŒ®åˆ†æžåŠ©æ‰‹ã€‚è¯·ä»”ç»†é˜…è¯»ä»¥ä¸‹è®ºæ–‡å†…å®¹ï¼Œå¹¶æŒ‰ç…§æŒ‡å®šçš„ç»“æž„è¿›è¡Œæ·±å…¥ã€å…¨é¢çš„åˆ†æžã€‚

## è®ºæ–‡åŸºæœ¬ä¿¡æ¯

- **æ ‡é¢˜**: {title}
- **ä½œè€…**: {authors}
- **æœŸåˆŠ**: {journal}
- **å‘è¡¨æ—¥æœŸ**: {date}
- **DOI**: {doi}

## è®ºæ–‡å…¨æ–‡

{fulltext}

{annotations_section}

---

## åˆ†æžè¦æ±‚

è¯·æŒ‰ç…§ä»¥ä¸‹ç»“æž„è¿›è¡Œè¯¦ç»†åˆ†æžï¼Œä»¥ Markdown æ ¼å¼è¿”å›žï¼š

### ðŸ“– ç²—è¯»ç­›é€‰

è¯·ä»Žä»¥ä¸‹ç»´åº¦å¿«é€Ÿè¯„ä¼°è¿™ç¯‡è®ºæ–‡çš„è´¨é‡å’Œé˜…è¯»ä»·å€¼ï¼š
- è®ºæ–‡å‘è¡¨æœŸåˆŠçš„å½±å“åŠ›å’Œé¢†åŸŸåœ°ä½
- ç ”ç©¶é—®é¢˜çš„é‡è¦æ€§å’Œå‰æ²¿æ€§
- æ–¹æ³•å’Œç»“è®ºçš„å¯é æ€§å’Œåˆ›æ–°æ€§
- **ç»“è®º**: æ˜¯å¦å»ºè®®æ·±å…¥é˜…è¯»ï¼Ÿé€‚åˆå“ªç±»ç ”ç©¶è€…ï¼Ÿ

### ðŸ“š å‰è¨€åŠæ–‡çŒ®ç»¼è¿°

#### å¼•ç”¨æ–‡çŒ®è¯„ä¼°
- å¼•ç”¨çš„æ–‡çŒ®æ˜¯å¦**æœ€æ–°**ã€**å…¨é¢**ï¼Ÿ
- ä»¥å¾€æ–‡çŒ®æœ‰ä»€ä¹ˆ**ä¸è¶³**æˆ–**ç ”ç©¶ç©ºç™½**ï¼Ÿ
- ä½œè€…å¦‚ä½•å®šä½æœ¬ç ”ç©¶ä¸Žå‰äººå·¥ä½œçš„å…³ç³»ï¼Ÿ

#### èšç„¦é—®é¢˜
- æœ¬ç ”ç©¶**èšç„¦çš„æ ¸å¿ƒç§‘å­¦é—®é¢˜**æ˜¯ä»€ä¹ˆï¼Ÿ
- ç ”ç©¶çš„**é€»è¾‘æ€è·¯**æ˜¯ä»€ä¹ˆï¼Ÿï¼ˆä»Žé—®é¢˜åˆ°æ–¹æ³•åˆ°ç»“è®ºçš„å®Œæ•´é“¾æ¡ï¼‰
- **å¯è¡Œæ€§**ï¼šæ–¹æ³•è®¾è®¡æ˜¯å¦åˆç†ï¼ŸæŠ€æœ¯è·¯çº¿æ˜¯å¦å¯è¡Œï¼Ÿ
- **å¯é æ€§**ï¼šå®žéªŒè®¾è®¡æ˜¯å¦ä¸¥è°¨ï¼Ÿå¯¹ç…§ç»„è®¾ç½®æ˜¯å¦åˆç†ï¼Ÿ

#### é€‰é¢˜æ–°é¢–æ€§
- ä½œè€…é€‰é¢˜è§’åº¦æ˜¯å¦**æ–°é¢–**ï¼Ÿ
- è¿™é¡¹ç ”ç©¶æœ‰ä»€ä¹ˆ**ç§‘å­¦ä»·å€¼**å’Œ**åº”ç”¨å‰æ™¯**ï¼Ÿ

### ðŸ’¡ åˆ›æ–°ç‚¹

è¯·ä»Žä»¥ä¸‹äº”ä¸ªç»´åº¦åˆ†æžåˆ›æ–°ç‚¹ï¼š

#### ç§‘å­¦é—®é¢˜
- æå‡ºäº†ä»€ä¹ˆæ–°çš„ç§‘å­¦é—®é¢˜æˆ–ç ”ç©¶è§†è§’ï¼Ÿ

#### åˆ¶å¤‡æ–¹æ³•  
- åœ¨ææ–™åˆ¶å¤‡æˆ–æ ·å“å‡†å¤‡ä¸Šæœ‰ä»€ä¹ˆåˆ›æ–°ï¼Ÿ
- æ˜¯å¦å¼€å‘äº†æ–°çš„åˆæˆè·¯çº¿æˆ–å·¥è‰ºï¼Ÿ

#### ç ”ç©¶æ€è·¯
- ç ”ç©¶è®¾è®¡æœ‰ä½•ç‹¬ç‰¹ä¹‹å¤„ï¼Ÿ
- æ˜¯å¦é‡‡ç”¨äº†æ–°çš„ç ”ç©¶èŒƒå¼æˆ–ç­–ç•¥ï¼Ÿ

#### ç ”ç©¶å·¥å…·
- ä½¿ç”¨äº†ä»€ä¹ˆæ–°çš„ç ”ç©¶å·¥å…·ã€æŠ€æœ¯æˆ–è¡¨å¾æ‰‹æ®µï¼Ÿ
- æ˜¯å¦å¼€å‘äº†æ–°çš„æµ‹è¯•æ–¹æ³•æˆ–åˆ†æžæ‰‹æ®µï¼Ÿ

#### ç ”ç©¶ç†è®º
- åœ¨ç†è®ºå±‚é¢æœ‰ä½•è´¡çŒ®ï¼Ÿ
- æ˜¯å¦æå‡ºäº†æ–°çš„æ¨¡åž‹ã€æœºåˆ¶è§£é‡Šæˆ–ç†è®ºæ¡†æž¶ï¼Ÿ

---

### âœ¨ ç¬”è®°åŽŸå­åŒ–

ä»¥ä¸‹éƒ¨åˆ†è¯·**æå–å…·ä½“æ•°æ®å’Œå…³é”®ä¿¡æ¯**ï¼Œä¾¿äºŽåŽç»­å¼•ç”¨ï¼š

#### ðŸ”§ åˆ¶å¤‡
- **åŽŸæ–™/å‰é©±ä½“**: åˆ—å‡ºå…³é”®ææ–™å’ŒåŒ–å­¦è¯•å‰‚
- **åˆ¶å¤‡æ­¥éª¤**: ç®€è¦æè¿°æ ¸å¿ƒå·¥è‰ºæµç¨‹ï¼ˆæ¸©åº¦ã€æ—¶é—´ã€æ°”æ°›ç­‰å…³é”®å‚æ•°ï¼‰
- **å…³é”®æ¡ä»¶**: å½±å“ç»“æžœçš„å…³é”®å®žéªŒæ¡ä»¶

#### ðŸ“Š è¡¨å¾
- **è¡¨å¾æ–¹æ³•**: ä½¿ç”¨äº†å“ªäº›è¡¨å¾æŠ€æœ¯ï¼Ÿï¼ˆXRD, SEM, TEM, XPS, BETç­‰ï¼‰
- **ä¸»è¦ç»“æžœ**: æ¯ç§è¡¨å¾æ–¹æ³•å¾—åˆ°çš„å…³é”®ç»“è®º
- **æ•°æ®æ”¯æŒ**: æå–å…³é”®çš„æ•°å€¼æ•°æ®ï¼ˆå¦‚æ™¶æ ¼å¸¸æ•°ã€ç²’å¾„ã€æ¯”è¡¨é¢ç§¯ç­‰ï¼‰

#### âš¡ æ€§èƒ½
- **æ€§èƒ½æŒ‡æ ‡**: åˆ—å‡ºå…³é”®æ€§èƒ½å‚æ•°ï¼ˆå¦‚æ´»æ€§ã€é€‰æ‹©æ€§ã€ç¨³å®šæ€§ã€æ•ˆçŽ‡ç­‰ï¼‰
- **å…·ä½“æ•°å€¼**: æå–å‡†ç¡®çš„æ•°å€¼å’Œå•ä½
- **å¯¹æ¯”åŸºå‡†**: ä¸Žæ–‡çŒ®æŠ¥é“æˆ–å•†ä¸šæ ‡å‡†çš„å¯¹æ¯”ç»“æžœ
- **ä¼˜åŠ¿ä½“çŽ°**: æ€§èƒ½ä¼˜åŠ¿ä½“çŽ°åœ¨å“ªäº›æ–¹é¢ï¼Ÿ

#### ðŸ”¬ æœºåˆ¶
- **æœºåˆ¶å‡è®¾**: ä½œè€…æå‡ºçš„ååº”æœºåˆ¶æˆ–ä½œç”¨æœºç†
- **è¯æ®æ”¯æŒ**: å“ªäº›å®žéªŒæ•°æ®æˆ–è¡¨å¾ç»“æžœæ”¯æŒè¿™ä¸€æœºåˆ¶ï¼Ÿ
- **å…³é”®æ­¥éª¤**: æœºåˆ¶ä¸­çš„å…³é”®ååº”æ­¥éª¤æˆ–ç‰©ç†åŒ–å­¦è¿‡ç¨‹
- **äº‰è®®ç‚¹**: æ˜¯å¦å­˜åœ¨å…¶ä»–å¯èƒ½çš„æœºåˆ¶è§£é‡Šï¼Ÿ

#### âœ¨ ç†è®º
- **ç†è®ºæ¨¡åž‹**: ä½¿ç”¨æˆ–å»ºç«‹äº†ä»€ä¹ˆç†è®ºæ¨¡åž‹ï¼Ÿ
- **è®¡ç®—æ–¹æ³•**: å¦‚æ¶‰åŠç†è®ºè®¡ç®—ï¼Œä½¿ç”¨äº†ä»€ä¹ˆæ–¹æ³•ï¼Ÿï¼ˆDFT, MDç­‰ï¼‰
- **ç†è®ºé¢„æµ‹**: ç†è®ºè®¡ç®—æˆ–æ¨¡åž‹é¢„æµ‹äº†ä»€ä¹ˆï¼Ÿ
- **å®žéªŒéªŒè¯**: ç†è®ºé¢„æµ‹æ˜¯å¦å¾—åˆ°å®žéªŒéªŒè¯ï¼Ÿ

---

### ðŸ¤” æ€è€ƒ

#### ä¼˜ç¼ºç‚¹åˆ†æž
- **ä¸»è¦ä¼˜ç‚¹**: è¿™ç¯‡è®ºæ–‡çš„çªå‡ºè´¡çŒ®ï¼ˆè‡³å°‘3ç‚¹ï¼‰
- **ä¸»è¦ç¼ºç‚¹**: å­˜åœ¨çš„é—®é¢˜æˆ–ä¸è¶³ï¼ˆè‡³å°‘2ç‚¹ï¼‰
  - å®žéªŒè®¾è®¡çš„å±€é™æ€§
  - æ•°æ®å®Œæ•´æ€§æˆ–è¯´æœåŠ›çš„æ¬ ç¼º
  - æœºåˆ¶è§£é‡Šçš„ä¸è¶³æˆ–äº‰è®®

#### ç–‘é—®ä¸Žäº‰è®®
- ä½ å¯¹å“ªäº›å†…å®¹äº§ç”Ÿäº†ç–‘é—®ï¼Ÿ
- å“ªäº›ç»“è®ºçš„å¯é æ€§éœ€è¦è¿›ä¸€æ­¥éªŒè¯ï¼Ÿ
- æ˜¯å¦å­˜åœ¨æ›¿ä»£æ€§è§£é‡Šæˆ–äº‰è®®æ€§è§‚ç‚¹ï¼Ÿ

#### ç ”ç©¶å¯å‘
- è¿™ç¯‡è®ºæ–‡ç»™ä½ å¸¦æ¥äº†ä»€ä¹ˆç ”ç©¶å¯å‘ï¼Ÿ
- å¯ä»¥å¦‚ä½•æ”¹è¿›æˆ–æ‰©å±•è¿™é¡¹å·¥ä½œï¼Ÿ
- å¯¹ä½ è‡ªå·±çš„ç ”ç©¶æœ‰ä»€ä¹ˆå€Ÿé‰´æ„ä¹‰ï¼Ÿ

---

### ðŸª¸ é‡ç»„åˆ†å­åŒ–ï¼ˆæ·±åº¦æ‰¹åˆ¤æ€§åˆ†æžï¼‰

è¯·å¯¹è®ºæ–‡çš„é€»è¾‘ä¸¥å¯†æ€§è¿›è¡Œæ‰¹åˆ¤æ€§è¯„ä¼°ï¼š

#### é€»è¾‘é“¾å®Œæ•´æ€§
- å›´ç»•æŸä¸€ä¸ªå®žéªŒçŽ°è±¡çš„é€»è¾‘é“¾æ˜¯å¦**å®Œæ•´**ã€**ä»¤äººä¿¡æœ**ï¼Ÿ
  - ä»Žé—®é¢˜æå‡º â†’ å®žéªŒè®¾è®¡ â†’ æ•°æ®å‘ˆçŽ° â†’ ç»“è®ºæŽ¨å¯¼
  - è®ºè¯è¿‡ç¨‹ä¸­æ˜¯å¦å­˜åœ¨**é€»è¾‘ç¼ºçŽ¯**ï¼Ÿ
  - æ˜¯å¦å­˜åœ¨**å…¶ä»–å¯èƒ½çš„è§£é‡Šè§’åº¦**ï¼Ÿ

#### æ•°æ®å¯ä¿¡åº¦
- å®žéªŒæ•°æ®çš„**å¯é‡å¤æ€§**å¦‚ä½•ï¼Ÿ
- å¯¹ç…§å®žéªŒæ˜¯å¦å……åˆ†ï¼Ÿ
- ç»Ÿè®¡åˆ†æžæ˜¯å¦è§„èŒƒï¼Ÿ
- æ•°æ®æ˜¯å¦æ”¯æŒä½œè€…çš„ç»“è®ºï¼Ÿ

#### ç»“è®ºåˆç†æ€§
- ç»“è®ºæ˜¯å¦**è¶…å‡ºæ•°æ®çš„æ”¯æŒèŒƒå›´**ï¼Ÿ
- æ˜¯å¦å­˜åœ¨**è¿‡åº¦è§£è¯»**æˆ–**è¿‡åº¦æŽ¨å¹¿**ï¼Ÿ
- ä½œè€…çš„æŽ¨æµ‹ä¸Žç¡®å‡¿è¯æ®ä¹‹é—´çš„ç•Œé™æ˜¯å¦æ¸…æ™°ï¼Ÿ

---

## è¾“å‡ºæ ¼å¼è¦æ±‚

1. **ä¿æŒå®¢è§‚ã€ä¸“ä¸šçš„åˆ†æžé£Žæ ¼**ï¼Œé¿å…ä¸»è§‚è‡†æ–­
2. **ä½¿ç”¨ä¸­æ–‡**æ’°å†™åˆ†æžå†…å®¹ï¼Œå…³é”®æœ¯è¯­å¯ç”¨è‹±æ–‡æ ‡æ³¨
3. **æå–å…·ä½“æ•°æ®**ï¼šæ•°å€¼ã€å•ä½ã€æ¡ä»¶ç­‰å…·ä½“ä¿¡æ¯
4. **å¼•ç”¨åŽŸæ–‡**ï¼šé‡è¦ç»“è®ºå¯ä»¥å¼•ç”¨åŽŸæ–‡è¡¨è¿°
5. **æ ‡æ³¨ä¸ç¡®å®šæ€§**ï¼šå¦‚æžœæŸä¸ªéƒ¨åˆ†åœ¨è®ºæ–‡ä¸­æ²¡æœ‰ç›¸å…³å†…å®¹ï¼Œè¯·æ˜Žç¡®æ³¨æ˜Ž"æœ¬æ–‡æœªæ¶‰åŠ"æˆ–"æ•°æ®ä¸è¶³"
6. **ä¿æŒç»“æž„å®Œæ•´**ï¼šå³ä½¿æŸéƒ¨åˆ†å†…å®¹è¾ƒå°‘ï¼Œä¹Ÿè¦ä¿ç•™æ ‡é¢˜ç»“æž„

---

**åˆ†æžç›®æ ‡**: å¸®åŠ©ç ”ç©¶è€…å¿«é€ŸæŠŠæ¡è®ºæ–‡æ ¸å¿ƒå†…å®¹ï¼Œæå–å¯å¼•ç”¨çš„å…³é”®ä¿¡æ¯ï¼Œå‘çŽ°æ½œåœ¨é—®é¢˜å’Œç ”ç©¶æœºä¼šã€‚
"""


# -------------------- LLM Client --------------------


class LLMClient:
    """
    Unified LLM client supporting multiple providers.

    Supports:
    - DeepSeek (OpenAI-compatible)
    - OpenAI
    - Google Gemini
    """

    def __init__(
        self,
        provider: Literal["deepseek", "openai", "gemini", "auto"] = "auto",
        model: str | None = None,
        api_key: str | None = None,
        base_url: str | None = None,
    ):
        """
        Initialize LLM client.

        Args:
            provider: LLM provider to use
            model: Model name (overrides default)
            api_key: API key (overrides env var)
            base_url: Base URL (overrides default)
        """
        self.provider = self._select_provider(provider)
        self.config = PROVIDERS[self.provider]

        # Get API key
        self.api_key = api_key or os.getenv(self.config["env_key"])
        if not self.api_key:
            raise ValueError(
                f"{self.provider.upper()} API key not found. "
                f"Set {self.config['env_key']} environment variable."
            )

        # Get base URL
        self.base_url = (
            base_url
            or os.getenv(self.config["env_base_url"])
            or self.config["base_url"]
        )

        # Get model
        self.model = (
            model or os.getenv(self.config["env_model"]) or self.config["default_model"]
        )

        logger.info(
            f"Initialized LLM client: provider={self.provider}, "
            f"model={self.model}, base_url={self.base_url}"
        )

    def _select_provider(self, provider: str) -> str:
        """Auto-select provider if set to 'auto'."""
        if provider != "auto":
            if provider not in PROVIDERS:
                raise ValueError(
                    f"Unknown provider: {provider}. Available: {list(PROVIDERS.keys())}"
                )
            return provider

        # Auto-select based on available API keys
        for prov, config in PROVIDERS.items():
            if os.getenv(config["env_key"]):
                logger.info(f"Auto-selected provider: {prov}")
                return prov

        raise ValueError(
            "No LLM API key found. Set one of: "
            + ", ".join(config["env_key"] for config in PROVIDERS.values())
        )

    async def analyze_paper(
        self,
        title: str,
        authors: str | None,
        journal: str | None,
        date: str | None,
        doi: str | None,
        fulltext: str,
        annotations: list[dict[str, Any]] | None = None,
        template: str | None = None,
    ) -> str:
        """
        Analyze a research paper and generate structured notes.

        Args:
            title: Paper title
            authors: Authors
            journal: Journal name
            date: Publication date
            doi: DOI
            fulltext: Full text content
            annotations: PDF annotations
            template: Custom analysis template/instruction

        Returns:
            Markdown-formatted analysis
        """
        # Build annotations section
        annotations_section = ""
        if annotations:
            annotations_section = "\n## PDF æ‰¹æ³¨\n\n"
            for i, ann in enumerate(annotations, 1):
                ann_type = ann.get("type", "note")
                text = ann.get("text", "")
                comment = ann.get("comment", "")
                page = ann.get("page", "")

                annotations_section += f"**æ‰¹æ³¨ {i}** ({ann_type}"
                if page:
                    annotations_section += f", ç¬¬{page}é¡µ"
                annotations_section += "):\n"

                if text:
                    annotations_section += f"> {text}\n"
                if comment:
                    annotations_section += f"*è¯„è®º*: {comment}\n"
                annotations_section += "\n"

        # Build prompt
        if template:
            # Use custom template strategy
            prompt = f"""ä½ æ˜¯ä¸€ä½ä¸“ä¸šçš„ç§‘ç ”æ–‡çŒ®åˆ†æžåŠ©æ‰‹ã€‚è¯·ä»”ç»†é˜…è¯»ä»¥ä¸‹è®ºæ–‡å†…å®¹ï¼Œå¹¶æŒ‰ç…§æä¾›çš„æ¨¡æ¿ç»“æž„è¿›è¡Œåˆ†æžã€‚

## è®ºæ–‡åŸºæœ¬ä¿¡æ¯

- **æ ‡é¢˜**: {title or "æœªçŸ¥"}
- **ä½œè€…**: {authors or "æœªçŸ¥"}
- **æœŸåˆŠ**: {journal or "æœªçŸ¥"}
- **å‘è¡¨æ—¥æœŸ**: {date or "æœªçŸ¥"}
- **DOI**: {doi or "æœªçŸ¥"}

## è®ºæ–‡å…¨æ–‡

{fulltext[:50000]}

{annotations_section}

---

## åˆ†æžè¦æ±‚

è¯·é˜…è¯»ä¸Šè¿°å†…å®¹ï¼Œå¹¶ä¸¥æ ¼æŒ‰ç…§ä»¥ä¸‹æ¨¡æ¿æ ¼å¼ç”Ÿæˆåˆ†æžæŠ¥å‘Šï¼š

{template}

**æ³¨æ„äº‹é¡¹**:
1. è¯·ä¿æŒå®¢è§‚ã€ä¸“ä¸šçš„åˆ†æžé£Žæ ¼
2. ä½¿ç”¨ä¸­æ–‡æ’°å†™åˆ†æžå†…å®¹
3. å¦‚æžœæ¨¡æ¿ä¸­æœ‰å ä½ç¬¦(å¦‚ ${{...}})ï¼Œè¯·æ›¿æ¢ä¸ºå®žé™…åˆ†æžå†…å®¹
4. å°½é‡æå–å…·ä½“çš„æ•°æ®ã€æ–¹æ³•å’Œç»“è®º
"""
        else:
            # Use default template
            prompt = ANALYSIS_TEMPLATE.format(
                title=title or "æœªçŸ¥",
                authors=authors or "æœªçŸ¥",
                journal=journal or "æœªçŸ¥",
                date=date or "æœªçŸ¥",
                doi=doi or "æœªçŸ¥",
                fulltext=fulltext[:50000],  # Limit to ~50k chars
                annotations_section=annotations_section,
            )

        # Call LLM
        if self.config["api_style"] == "openai":
            return await self._call_openai_style(prompt)
        elif self.config["api_style"] == "google":
            return await self._call_google_style(prompt)
        else:
            raise ValueError(f"Unknown API style: {self.config['api_style']}")

    async def _call_openai_style(self, prompt: str) -> str:
        """Call OpenAI-compatible API (DeepSeek, OpenAI)."""
        try:
            from openai import AsyncOpenAI
        except ImportError as e:
            raise ImportError(
                "openai package not installed. Install with: pip install openai"
            ) from e

        client = AsyncOpenAI(api_key=self.api_key, base_url=self.base_url)

        try:
            response = await client.chat.completions.create(
                model=self.model,
                messages=[
                    {
                        "role": "system",
                        "content": "ä½ æ˜¯ä¸€ä½ä¸“ä¸šçš„ç§‘ç ”æ–‡çŒ®åˆ†æžåŠ©æ‰‹ï¼Œæ“…é•¿æ·±å…¥åˆ†æžå­¦æœ¯è®ºæ–‡å¹¶æå–å…³é”®ä¿¡æ¯ã€‚",
                    },
                    {"role": "user", "content": prompt},
                ],
                temperature=0.7,
                max_tokens=4000,
            )

            return response.choices[0].message.content or ""

        except Exception as e:
            logger.error(f"LLM API call failed: {e}")
            raise

    async def _call_google_style(self, prompt: str) -> str:
        """Call Google Gemini API."""
        try:
            import google.generativeai as genai
        except ImportError as e:
            raise ImportError(
                "google-generativeai package not installed. "
                "Install with: pip install google-generativeai"
            ) from e

        genai.configure(api_key=self.api_key)

        model = genai.GenerativeModel(self.model)

        try:
            loop = asyncio.get_event_loop()
            response = await loop.run_in_executor(
                None, lambda: model.generate_content(prompt)
            )

            return response.text

        except Exception as e:
            logger.error(f"Gemini API call failed: {e}")
            raise


# -------------------- Helper Functions --------------------


def get_llm_client(
    provider: str = "auto",
    model: str | None = None,
) -> LLMClient:
    """
    Get configured LLM client.

    Args:
        provider: Provider name or "auto"
        model: Model name (optional)

    Returns:
        Configured LLMClient
    """
    return LLMClient(provider=provider, model=model)


def is_llm_configured() -> bool:
    """Check if any LLM API is configured."""
    for config in PROVIDERS.values():
        if os.getenv(config["env_key"]):
            return True
    return False


def get_configured_provider() -> str | None:
    """Get the first configured provider."""
    for prov, config in PROVIDERS.items():
        if os.getenv(config["env_key"]):
            return prov
    return None
