"""
LLM client for Zotero MCP.

Provides unified interface for calling LLM APIs (DeepSeek, OpenAI, Gemini)
to analyze research papers and generate structured notes.
"""

import asyncio
import logging
import os
from typing import Any, Literal

logger = logging.getLogger(__name__)


# -------------------- Provider Configuration --------------------


PROVIDERS = {
    "deepseek": {
        "base_url": "https://api.deepseek.com",
        "default_model": "deepseek-chat",
        "api_style": "openai",  # OpenAI-compatible API
        "env_key": "DEEPSEEK_API_KEY",
        "env_base_url": "DEEPSEEK_BASE_URL",
        "env_model": "DEEPSEEK_MODEL",
    },
    "openai": {
        "base_url": "https://api.openai.com/v1",
        "default_model": "gpt-4o-mini",
        "api_style": "openai",
        "env_key": "OPENAI_API_KEY",
        "env_base_url": "OPENAI_BASE_URL",
        "env_model": "OPENAI_MODEL",
    },
    "gemini": {
        "base_url": "https://generativelanguage.googleapis.com",
        "default_model": "gemini-1.5-flash",
        "api_style": "google",
        "env_key": "GEMINI_API_KEY",
        "env_base_url": "GEMINI_BASE_URL",
        "env_model": "GEMINI_MODEL",
    },
}


# -------------------- Analysis Template --------------------


ANALYSIS_TEMPLATE = """ä½ æ˜¯ä¸€ä½ä¸“ä¸šçš„ç§‘ç ”æ–‡çŒ®åˆ†æžåŠ©æ‰‹ã€‚è¯·ä»”ç»†é˜…è¯»ä»¥ä¸‹è®ºæ–‡å†…å®¹ï¼Œå¹¶æŒ‰ç…§æŒ‡å®šçš„ç»“æž„è¿›è¡Œåˆ†æžã€‚

## è®ºæ–‡åŸºæœ¬ä¿¡æ¯

- **æ ‡é¢˜**: {title}
- **ä½œè€…**: {authors}
- **æœŸåˆŠ**: {journal}
- **å‘è¡¨æ—¥æœŸ**: {date}
- **DOI**: {doi}

## è®ºæ–‡å…¨æ–‡

{fulltext}

{annotations_section}

---

## åˆ†æžè¦æ±‚

è¯·æŒ‰ç…§ä»¥ä¸‹ç»“æž„è¿›è¡Œè¯¦ç»†åˆ†æžï¼Œä»¥ Markdown æ ¼å¼è¿”å›žï¼š

### ðŸ“– ç²—è¯»ç­›é€‰
- ç®€è¦è¯„ä¼°è¿™ç¯‡è®ºæ–‡çš„è´¨é‡å’Œé˜…è¯»ä»·å€¼

### ðŸ“š å‰è¨€åŠæ–‡çŒ®ç»¼è¿°
- **å¼•ç”¨æ–‡çŒ®è¯„ä¼°**: å¼•ç”¨çš„æ–‡çŒ®æ˜¯å¦æœ€æ–°ã€å…¨é¢ï¼Ÿä»¥å¾€æ–‡çŒ®æœ‰ä»€ä¹ˆä¸è¶³ï¼Ÿ
- **èšç„¦é—®é¢˜**: æœ¬ç ”ç©¶èšç„¦çš„é—®é¢˜æ˜¯ä»€ä¹ˆï¼Ÿé€»è¾‘æ€è·¯ã€å¯è¡Œæ€§å’Œå¯é æ€§å¦‚ä½•ï¼Ÿ
- **é€‰é¢˜æ–°é¢–æ€§**: ä½œè€…é€‰é¢˜è§’åº¦æ˜¯å¦æ–°é¢–ï¼Ÿæœ‰ä»€ä¹ˆä»·å€¼ï¼Ÿ

### ðŸ’¡ åˆ›æ–°ç‚¹
- **ç§‘å­¦é—®é¢˜**: æå‡ºäº†ä»€ä¹ˆæ–°çš„ç§‘å­¦é—®é¢˜ï¼Ÿ
- **åˆ¶å¤‡æ–¹æ³•**: åœ¨åˆ¶å¤‡æ–¹æ³•ä¸Šæœ‰ä»€ä¹ˆåˆ›æ–°ï¼Ÿ
- **ç ”ç©¶æ€è·¯**: ç ”ç©¶æ€è·¯æœ‰ä½•ç‹¬ç‰¹ä¹‹å¤„ï¼Ÿ
- **ç ”ç©¶å·¥å…·**: ä½¿ç”¨äº†ä»€ä¹ˆæ–°çš„ç ”ç©¶å·¥å…·æˆ–æŠ€æœ¯ï¼Ÿ
- **ç ”ç©¶ç†è®º**: åœ¨ç†è®ºæ–¹é¢æœ‰ä½•è´¡çŒ®ï¼Ÿ

### âœ¨ ç¬”è®°åŽŸå­åŒ–

#### ðŸ”§ åˆ¶å¤‡
- æå–å…³é”®çš„åˆ¶å¤‡æ–¹æ³•å’Œæ­¥éª¤

#### ðŸ“Š è¡¨å¾
- æ€»ç»“ä½¿ç”¨çš„è¡¨å¾æ–¹æ³•å’Œä¸»è¦ç»“æžœ

#### âš¡ æ€§èƒ½
- æå–å…³é”®æ€§èƒ½æ•°æ®å’ŒæŒ‡æ ‡

#### ðŸ”¬ æœºåˆ¶
- é˜è¿°ä½œè€…æå‡ºçš„æœºåˆ¶è§£é‡Š

#### âœ¨ ç†è®º
- æ€»ç»“ç†è®ºåŸºç¡€å’Œæ¨¡åž‹

### ðŸ¤” æ€è€ƒ
- **ä¼˜ç¼ºç‚¹**: è¿™ç¯‡è®ºæ–‡æœ‰ä»€ä¹ˆä¼˜ç‚¹å’Œä¸è¶³ï¼Ÿ
- **ç–‘é—®**: ä½ å¯¹å“ªäº›å†…å®¹äº§ç”Ÿäº†ç–‘é—®ï¼Ÿ
- **å¯å‘**: è¿™ç¯‡è®ºæ–‡ç»™ä½ å¸¦æ¥äº†ä»€ä¹ˆç ”ç©¶å¯å‘ï¼Ÿ

---

**æ³¨æ„äº‹é¡¹**:
1. è¯·ä¿æŒå®¢è§‚ã€ä¸“ä¸šçš„åˆ†æžé£Žæ ¼
2. ä½¿ç”¨ä¸­æ–‡æ’°å†™åˆ†æžå†…å®¹
3. å…³é”®æœ¯è¯­å¯ä»¥ç”¨è‹±æ–‡æ ‡æ³¨
4. å¦‚æžœæŸä¸ªéƒ¨åˆ†åœ¨è®ºæ–‡ä¸­æ²¡æœ‰ç›¸å…³å†…å®¹ï¼Œè¯·æ³¨æ˜Ž"æœ¬æ–‡æœªæ¶‰åŠ"
5. å°½é‡æå–å…·ä½“çš„æ•°æ®ã€æ–¹æ³•å’Œç»“è®º
"""


# -------------------- LLM Client --------------------


class LLMClient:
    """
    Unified LLM client supporting multiple providers.

    Supports:
    - DeepSeek (OpenAI-compatible)
    - OpenAI
    - Google Gemini
    """

    def __init__(
        self,
        provider: Literal["deepseek", "openai", "gemini", "auto"] = "auto",
        model: str | None = None,
        api_key: str | None = None,
        base_url: str | None = None,
    ):
        """
        Initialize LLM client.

        Args:
            provider: LLM provider to use
            model: Model name (overrides default)
            api_key: API key (overrides env var)
            base_url: Base URL (overrides default)
        """
        self.provider = self._select_provider(provider)
        self.config = PROVIDERS[self.provider]

        # Get API key
        self.api_key = api_key or os.getenv(self.config["env_key"])
        if not self.api_key:
            raise ValueError(
                f"{self.provider.upper()} API key not found. "
                f"Set {self.config['env_key']} environment variable."
            )

        # Get base URL
        self.base_url = (
            base_url
            or os.getenv(self.config["env_base_url"])
            or self.config["base_url"]
        )

        # Get model
        self.model = (
            model or os.getenv(self.config["env_model"]) or self.config["default_model"]
        )

        logger.info(
            f"Initialized LLM client: provider={self.provider}, "
            f"model={self.model}, base_url={self.base_url}"
        )

    def _select_provider(self, provider: str) -> str:
        """Auto-select provider if set to 'auto'."""
        if provider != "auto":
            if provider not in PROVIDERS:
                raise ValueError(
                    f"Unknown provider: {provider}. Available: {list(PROVIDERS.keys())}"
                )
            return provider

        # Auto-select based on available API keys
        for prov, config in PROVIDERS.items():
            if os.getenv(config["env_key"]):
                logger.info(f"Auto-selected provider: {prov}")
                return prov

        raise ValueError(
            "No LLM API key found. Set one of: "
            + ", ".join(config["env_key"] for config in PROVIDERS.values())
        )

    async def analyze_paper(
        self,
        title: str,
        authors: str | None,
        journal: str | None,
        date: str | None,
        doi: str | None,
        fulltext: str,
        annotations: list[dict[str, Any]] | None = None,
    ) -> str:
        """
        Analyze a research paper and generate structured notes.

        Args:
            title: Paper title
            authors: Authors
            journal: Journal name
            date: Publication date
            doi: DOI
            fulltext: Full text content
            annotations: PDF annotations

        Returns:
            Markdown-formatted analysis
        """
        # Build annotations section
        annotations_section = ""
        if annotations:
            annotations_section = "\n## PDF æ‰¹æ³¨\n\n"
            for i, ann in enumerate(annotations, 1):
                ann_type = ann.get("type", "note")
                text = ann.get("text", "")
                comment = ann.get("comment", "")
                page = ann.get("page", "")

                annotations_section += f"**æ‰¹æ³¨ {i}** ({ann_type}"
                if page:
                    annotations_section += f", ç¬¬{page}é¡µ"
                annotations_section += "):\n"

                if text:
                    annotations_section += f"> {text}\n"
                if comment:
                    annotations_section += f"*è¯„è®º*: {comment}\n"
                annotations_section += "\n"

        # Build prompt
        prompt = ANALYSIS_TEMPLATE.format(
            title=title or "æœªçŸ¥",
            authors=authors or "æœªçŸ¥",
            journal=journal or "æœªçŸ¥",
            date=date or "æœªçŸ¥",
            doi=doi or "æœªçŸ¥",
            fulltext=fulltext[:50000],  # Limit to ~50k chars
            annotations_section=annotations_section,
        )

        # Call LLM
        if self.config["api_style"] == "openai":
            return await self._call_openai_style(prompt)
        elif self.config["api_style"] == "google":
            return await self._call_google_style(prompt)
        else:
            raise ValueError(f"Unknown API style: {self.config['api_style']}")

    async def _call_openai_style(self, prompt: str) -> str:
        """Call OpenAI-compatible API (DeepSeek, OpenAI)."""
        try:
            from openai import AsyncOpenAI
        except ImportError:
            raise ImportError(
                "openai package not installed. Install with: pip install openai"
            )

        client = AsyncOpenAI(api_key=self.api_key, base_url=self.base_url)

        try:
            response = await client.chat.completions.create(
                model=self.model,
                messages=[
                    {
                        "role": "system",
                        "content": "ä½ æ˜¯ä¸€ä½ä¸“ä¸šçš„ç§‘ç ”æ–‡çŒ®åˆ†æžåŠ©æ‰‹ï¼Œæ“…é•¿æ·±å…¥åˆ†æžå­¦æœ¯è®ºæ–‡å¹¶æå–å…³é”®ä¿¡æ¯ã€‚",
                    },
                    {"role": "user", "content": prompt},
                ],
                temperature=0.7,
                max_tokens=4000,
            )

            return response.choices[0].message.content or ""

        except Exception as e:
            logger.error(f"LLM API call failed: {e}")
            raise

    async def _call_google_style(self, prompt: str) -> str:
        """Call Google Gemini API."""
        try:
            import google.generativeai as genai
        except ImportError:
            raise ImportError(
                "google-generativeai package not installed. "
                "Install with: pip install google-generativeai"
            )

        genai.configure(api_key=self.api_key)

        model = genai.GenerativeModel(self.model)

        try:
            loop = asyncio.get_event_loop()
            response = await loop.run_in_executor(
                None, lambda: model.generate_content(prompt)
            )

            return response.text

        except Exception as e:
            logger.error(f"Gemini API call failed: {e}")
            raise


# -------------------- Helper Functions --------------------


def get_llm_client(
    provider: str = "auto",
    model: str | None = None,
) -> LLMClient:
    """
    Get configured LLM client.

    Args:
        provider: Provider name or "auto"
        model: Model name (optional)

    Returns:
        Configured LLMClient
    """
    return LLMClient(provider=provider, model=model)


def is_llm_configured() -> bool:
    """Check if any LLM API is configured."""
    for config in PROVIDERS.values():
        if os.getenv(config["env_key"]):
            return True
    return False


def get_configured_provider() -> str | None:
    """Get the first configured provider."""
    for prov, config in PROVIDERS.items():
        if os.getenv(config["env_key"]):
            return prov
    return None
